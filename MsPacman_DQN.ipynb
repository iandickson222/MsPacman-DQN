{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MsPacman DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9I7p+aRdwSB4LfNsH3eCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iandickson222/MsPacman-DQN/blob/main/MsPacman_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQTg1GANwv43"
      },
      "source": [
        "%%bash\n",
        "sudo apt-get install -y xvfb ffmpeg\n",
        "pip install -q 'imageio==2.4.0'\n",
        "pip install -q pyvirtualdisplay\n",
        "pip install -q tf-agents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93RUP01fwysM"
      },
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import os\n",
        "import collections\n",
        "import gym\n",
        "\n",
        "import tf_agents\n",
        "import tensorflow as tf\n",
        "from tf_agents.environments import suite_atari\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXvbkzHmlwXA"
      },
      "source": [
        "class ObservationCollector(gym.Wrapper):\n",
        "\n",
        "  def __init__(self, env):\n",
        "    super(ObservationCollector, self).__init__(env)\n",
        "    self._observations = collections.deque(maxlen=50000)\n",
        "    \n",
        "  def step(self, action):\n",
        "    observation, accumulated_reward, is_terminal, info = self.env.step(action)\n",
        "    self._observations.append(observation) \n",
        "    return observation, accumulated_reward, is_terminal, info\n",
        "  \n",
        "  def reset(self):\n",
        "    observation = self.env.reset()\n",
        "    self._observations.clear()\n",
        "    self._observations.append(observation)\n",
        "    return observation\n",
        "  \n",
        "  def return_observations(self):\n",
        "    return self._observations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W27iRprPvPU"
      },
      "source": [
        "class Normalizer(tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n",
        "\n",
        "  def __init__(self, env, start_assistance = False):\n",
        "    super(Normalizer, self).__init__(env)\n",
        "    self._env = env\n",
        "    self._observation_spec = tf_agents.specs.BoundedArraySpec(\n",
        "        shape = env.observation_spec().shape,\n",
        "        dtype = np.float32,\n",
        "        minimum = 0.0,\n",
        "        maximum = 1.0,\n",
        "        name = env.observation_spec().name)\n",
        "    \n",
        "  def _step(self, action):\n",
        "    time_step = self._env.step(action)  \n",
        "    observation = time_step.observation.astype('float32')\n",
        "    time_step = time_step._replace(observation = observation/255.0)\n",
        "    return time_step\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "  \n",
        "  def _reset(self):\n",
        "    time_step = self._env.reset()\n",
        "    observation = time_step.observation.astype('float32')\n",
        "    time_step = time_step._replace(observation = observation/255.0)\n",
        "    return time_step"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnam98DOw7oR"
      },
      "source": [
        "environment_name = \"MsPacman-v0\"\n",
        "dir = \"drive/MyDrive/PacmanDQN\"\n",
        "\n",
        "train_py_env = suite_atari.load(\n",
        "    environment_name, \n",
        "    gym_env_wrappers = suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING, \n",
        "    env_wrappers=(Normalizer,))\n",
        "\n",
        "test_py_env = suite_atari.load(\n",
        "    environment_name,\n",
        "    gym_env_wrappers = (ObservationCollector,) + suite_atari.DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING, \n",
        "    env_wrappers = (Normalizer,))\n",
        "\n",
        "train_tf_env = tf_agents.environments.tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "test_tf_env = tf_agents.environments.tf_py_environment.TFPyEnvironment(test_py_env)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMFTfQgyIeI"
      },
      "source": [
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "q_net = tf_agents.networks.q_network.QNetwork(\n",
        "    input_tensor_spec = train_tf_env.observation_spec(),\n",
        "    action_spec = train_tf_env.action_spec(),\n",
        "    conv_layer_params = ((32, 8, 4), (64, 4, 2), (64, 3, 1)), \n",
        "    fc_layer_params = (512,))\n",
        "\n",
        "agent = tf_agents.agents.DqnAgent(\n",
        "    train_tf_env.time_step_spec(),\n",
        "    train_tf_env.action_spec(),\n",
        "    q_network = q_net,\n",
        "    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.0003),\n",
        "    epsilon_greedy = 0.03,\n",
        "    n_step_update = 2,\n",
        "    target_update_tau = 0.005,\n",
        "    td_errors_loss_fn = tf_agents.utils.common.element_wise_huber_loss,\n",
        "    gamma = 0.99,\n",
        "    train_step_counter = global_step)\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-1iWC5Lw29x"
      },
      "source": [
        "replay_buffer = tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec = agent.collect_data_spec,\n",
        "    batch_size = train_tf_env.batch_size,\n",
        "    max_length = 10000)\n",
        "\n",
        "dataset = replay_buffer.as_dataset(sample_batch_size = 64, num_steps = 3, num_parallel_calls = 3).prefetch(3)\n",
        "dataset = iter(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ3m3hur6Ot6"
      },
      "source": [
        "number_episodes_metric = tf_agents.metrics.tf_metrics.NumberOfEpisodes()\n",
        "average_return_metric = tf_agents.metrics.tf_metrics.AverageReturnMetric()\n",
        "random_policy = tf_agents.policies.random_tf_policy.RandomTFPolicy(train_tf_env.time_step_spec(), train_tf_env.action_spec())\n",
        "\n",
        "train_driver = tf_agents.drivers.dynamic_step_driver.DynamicStepDriver(\n",
        "    env = train_tf_env,\n",
        "    policy = agent.collect_policy,\n",
        "    observers = [replay_buffer.add_batch, number_episodes_metric],\n",
        "    num_steps = 10)\n",
        "\n",
        "test_driver = tf_agents.drivers.dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "    env = test_tf_env,\n",
        "    policy = agent.policy,\n",
        "    observers = [average_return_metric],\n",
        "    num_episodes = 1)\n",
        "\n",
        "random_driver = tf_agents.drivers.dynamic_step_driver.DynamicStepDriver(\n",
        "    env = train_tf_env,\n",
        "    policy = random_policy,\n",
        "    observers = [replay_buffer.add_batch],\n",
        "    num_steps = 1000)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI59gwVOw9Ww"
      },
      "source": [
        "checkpoint_dir = os.path.join(os.getcwd(), f'{dir}/checkpoint')\n",
        "\n",
        "train_checkpointer = tf_agents.utils.common.Checkpointer(\n",
        "    ckpt_dir = checkpoint_dir,\n",
        "    max_to_keep = 1,\n",
        "    agent = agent,\n",
        "    policy = agent.policy,\n",
        "    replay_buffer = replay_buffer,\n",
        "    global_step = global_step)\n",
        "\n",
        "train_checkpointer.initialize_or_restore()\n",
        "global_step = tf.compat.v1.train.get_global_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFCNWjU2Q_Eu"
      },
      "source": [
        "def capture_episodes(video_filename, num_episodes = 1):\n",
        "    with imageio.get_writer(video_filename, fps = 24) as video:\n",
        "      for _ in range(num_episodes):\n",
        "        time_step = test_tf_env.reset()\n",
        "        while not time_step.is_last():\n",
        "          policy_step = agent.policy.action(time_step)\n",
        "          time_step = test_tf_env.step(policy_step.action)\n",
        "\n",
        "        for observation in test_py_env.return_observations():\n",
        "          video.append_data(observation)    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYuvycSOwzG4"
      },
      "source": [
        "if global_step == 0:\n",
        "  random_driver.run()\n",
        "  os.makedirs(f'{dir}/videos')   \n",
        "  capture_episodes(f'{dir}/videos/no_training.mp4')\n",
        "  \n",
        "step = global_step\n",
        "time_step = train_tf_env.reset()\n",
        "agent.train = tf_agents.utils.common.function(agent.train)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSFEeQBZw_xQ"
      },
      "source": [
        "for epoch in range(step + 1, step + 100001):\n",
        "  time_step, _ = train_driver.run(time_step)\n",
        "  experience, _ = next(dataset)\n",
        "  loss, _ = agent.train(experience)\n",
        "\n",
        "  if epoch % 1000  == 0:\n",
        "    test_driver.run()\n",
        "    num_episodes = number_episodes_metric.result().numpy()\n",
        "    test_score = average_return_metric.result().numpy() \n",
        "    average_return_metric.reset()\n",
        "    capture_episodes(f'{dir}/videos/epoch_{epoch}_episode_{num_episodes+6848}_score_{test_score}.mp4')\n",
        "    train_checkpointer.save(global_step)\n",
        "    step = epoch\n",
        "    print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}